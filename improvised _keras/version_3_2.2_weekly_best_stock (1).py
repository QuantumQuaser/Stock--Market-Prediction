# -*- coding: utf-8 -*-
"""Version_3.2.weekly best_stock.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFCOyUla7apsF3jIASMXg7Ew7izV_OBM
"""

import yfinance as yf

def download_stock_data(ticker, start_date, end_date):
    return yf.download(ticker, start=start_date, end=end_date)

stocks = ['BAJFINANCE.NS', 'HDFCAMC.NS', 'ASIANPAINT.NS', 'TCS.NS', 'DRREDDY.NS', '^NSEI']  # ^NSEI for Nifty 50
start_date = '2012-01-01'
end_date = '2023-01-26'

stock_data = {stock: download_stock_data(stock, start_date, end_date) for stock in stocks}

pip install feedparser textblob

import feedparser
from textblob import TextBlob
import pandas as pd

def fetch_news(stock_name):
    feed_url = f'https://news.google.com/rss/search?q={stock_name}+when:7d&hl=en-IN&gl=IN&ceid=IN:en'
    news_feed = feedparser.parse(feed_url)
    return [(entry.title, entry.link, entry.published) for entry in news_feed.entries]

def analyze_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

def get_stock_news_sentiment(stock_names):
    all_news_sentiments = []
    for stock in stock_names:
        news_items = fetch_news(stock)
        for title, link, published in news_items:
            sentiment_score = analyze_sentiment(title)  # Analyzing sentiment of the news title
            all_news_sentiments.append([stock, title, link, published, sentiment_score])
    return pd.DataFrame(all_news_sentiments, columns=['Stock', 'Title', 'Link', 'Published', 'Sentiment'])

# List of stocks
stocks = ['BAJFINANCE', 'HDFCAMC', 'ASIANPAINT', 'TCS', 'DRREDDY']

# Get news sentiment
news_sentiment = get_stock_news_sentiment(stocks)
print(news_sentiment)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from textblob import TextBlob
import numpy as np

def get_stock_data(ticker, start_date, end_date):
    return yf.download(ticker, start=start_date, end=end_date)

def calculate_rsi(data, window=14):
    delta = data['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(0)

def calculate_sma(data, window=10):
    return data['Close'].rolling(window=window).mean()

def calculate_ema(data, window=10):
    return data['Close'].ewm(span=window, adjust=False).mean()

def perform_sentiment_analysis(text):
    return TextBlob(text).sentiment.polarity

tickers = ['BAJFINANCE.NS', 'HDFCAMC.NS', 'ASIANPAINT.NS', 'TCS.NS', 'DRREDDY.NS']
start_date = '2012-01-01'
end_date = '2023-12-26'

stock_data = {}
for ticker in tickers:
    df = get_stock_data(ticker, start_date, end_date)
    df['RSI'] = calculate_rsi(df)
    df['SMA'] = calculate_sma(df)
    df['EMA'] = calculate_ema(df)
    stock_data[ticker] = df

for ticker in stock_data:
    # Fetch news and analyze sentiment
    news_items = fetch_news(ticker)
    sentiment_scores = [analyze_sentiment(title) for title, _, _ in news_items]
    avg_sentiment_score = np.mean(sentiment_scores) if sentiment_scores else 0

    # Add Avg_Sentiment to the DataFrame
    stock_data[ticker]['Avg_Sentiment'] = avg_sentiment_score

#tweaked nifty data added
start_date = '2012-01-01'
end_date = '2023-12-26'
nifty_data = get_stock_data('^NSEI', start_date, end_date)  # '^NSEI' is the ticker for Nifty 50

nifty_data['RSI_Nifty'] = calculate_rsi(nifty_data)
nifty_data['SMA_Nifty'] = calculate_sma(nifty_data)
nifty_data['EMA_Nifty'] = calculate_ema(nifty_data)

for ticker in stock_data:
    stock_data[ticker] = stock_data[ticker].merge(nifty_data[['RSI_Nifty', 'SMA_Nifty', 'EMA_Nifty']], left_index=True, right_index=True, how='left')

def calculate_weekly_return(data):
    return data['Close'].pct_change(periods=5)  # 5 trading days in a week

for ticker in stock_data:
    stock_data[ticker]['Week'] = stock_data[ticker].index.to_period('W')
    stock_data[ticker]['Weekly_Return'] = stock_data[ticker]['Close'].pct_change(periods=5)
    stock_data[ticker]['Ticker'] = ticker

combined_data = pd.concat(stock_data.values())
combined_data.dropna(subset=['Weekly_Return'], inplace=True)

weekly_returns = combined_data.groupby(['Week', 'Ticker'])['Weekly_Return'].mean().reset_index()

# Make sure there are no NaN values in the 'Week' and 'Weekly_Return' columns
weekly_returns.dropna(subset=['Week', 'Weekly_Return'], inplace=True)

# Determine the best-performing stock for each week
weekly_best = weekly_returns.loc[weekly_returns.groupby('Week')['Weekly_Return'].idxmax()]
weekly_best.rename(columns={'Ticker': 'Best_Ticker', 'Weekly_Return': 'Best_Weekly_Return'}, inplace=True)

# Merge to identify the best-performing stock each week
combined_data = pd.merge(combined_data, weekly_best[['Week', 'Best_Ticker']], on='Week', how='left')

# Create the target variable
combined_data['Target'] = (combined_data['Ticker'] == combined_data['Best_Ticker']).astype(int)

## bes
# Convert to a numerical format for modeling
# Ensure this mapping covers all your tickers
target_mapping = {ticker: idx for idx, ticker in enumerate(tickers)}
combined_data['Target'] = combined_data['Ticker'].map(target_mapping)

combined_data = pd.concat(stock_data.values())
scaler = StandardScaler()
features = ['RSI', 'SMA', 'EMA', 'Avg_Sentiment','RSI_Nifty', 'SMA_Nifty', 'EMA_Nifty']
combined_data[features] = scaler.fit_transform(combined_data[features])

combined_data['Target'] = combined_data['Close'].shift(-5) > combined_data['Close']
X = combined_data[features].fillna(0)
y = combined_data['Target'].fillna(False)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

predictions = model.predict(X_test)
predicted_stock_indices = pd.Series(predictions, index=X_test.index)

# Map back to stock tickers
predicted_stock_names = predicted_stock_indices.map({v: k for k, v in target_mapping.items()})

# Now, predicted_stock_names contains the tickers of the predicted best-performing stocks
inverse_target_mapping = {v: k for k, v in target_mapping.items()}

from sklearn.metrics import classification_report, accuracy_score
print("Accuracy:", accuracy_score(y_test, predictions))

unique_labels = combined_data['Target'].unique()
target_names = [inverse_target_mapping[label] for label in unique_labels]

print(classification_report(y_test, predictions, labels=unique_labels, target_names=target_names))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
print(classification_report(y_test, predictions))