# -*- coding: utf-8 -*-
"""Version_4.1_keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L1cfgC0CVk8iEsO347KeT1wlHRVrNaqr
"""

# Install and Import Necessary Libraries
!pip install yfinance feedparser textblob keras tensorflow

import yfinance as yf
import pandas as pd
import numpy as np
import feedparser
from textblob import TextBlob
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score
from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_auc_score, confusion_matrix
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.optimizers import Adam

# Function Definitions
def download_stock_data(ticker, start_date, end_date):
    return yf.download(ticker, start=start_date, end=end_date)

def fetch_news(stock_name):
    feed_url = f'https://news.google.com/rss/search?q={stock_name}+when:7d&hl=en-IN&gl=IN&ceid=IN:en'
    news_feed = feedparser.parse(feed_url)
    return [(entry.title, entry.link, entry.published) for entry in news_feed.entries]

def analyze_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

def calculate_rsi(data, window=10):
    delta = data['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs)).fillna(0)

def calculate_sma(data, window=10):
    return data['Close'].rolling(window=window).mean().fillna(0)

def calculate_ema(data, window=10):
    return data['Close'].ewm(span=window, adjust=False).mean().fillna(0)

# Download Stock Data
stocks = ['BAJFINANCE.NS', 'HDFCAMC.NS', 'ASIANPAINT.NS', 'TCS.NS', 'DRREDDY.NS', '^NSEI']
start_date = '2012-01-01'
end_date = '2023-01-26'
stock_data = {stock: download_stock_data(stock, start_date, end_date) for stock in stocks}

# Process Data for Each Stock
all_data = []
for ticker in stocks[:-1]:  # Exclude Nifty 50
    df = stock_data[ticker]
    df['RSI'] = calculate_rsi(df)
    df['SMA'] = calculate_sma(df)
    df['EMA'] = calculate_ema(df)
    news_items = fetch_news(ticker)
    sentiment_scores = [analyze_sentiment(title) for title, _, _ in news_items]
    avg_sentiment_score = np.mean(sentiment_scores) if sentiment_scores else 0
    df['Avg_Sentiment'] = avg_sentiment_score
    df['Ticker'] = ticker
    all_data.append(df)

# Combine All Stock Data
combined_data = pd.concat(all_data)

# Process Nifty 50 Data
nifty_data = stock_data['^NSEI']
nifty_data['RSI_Nifty'] = calculate_rsi(nifty_data)
nifty_data['SMA_Nifty'] = calculate_sma(nifty_data)
nifty_data['EMA_Nifty'] = calculate_ema(nifty_data)

# Create 'Target' Column
combined_data['Future_Return'] = combined_data.groupby('Ticker')['Close'].transform(lambda x: x.pct_change(periods=7).shift(-7))
combined_data['Max_Return'] = combined_data.groupby('Date')['Future_Return'].transform('max')
combined_data['Target'] = (combined_data['Future_Return'] == combined_data['Max_Return']).astype(int)

# Drop rows with NaN values in 'Target' or features
combined_data.dropna(subset=['Target'] + features, inplace=True)

# Merge Nifty Data with Stock Data
#combined_data = combined_data.merge(nifty_data[['RSI_Nifty', 'SMA_Nifty', 'EMA_Nifty']], left_index=True, right_index=True, how='left')

# Feature Scaling and Data Preparation
scaler = StandardScaler()
features = ['RSI', 'SMA', 'EMA', 'Avg_Sentiment', 'RSI_Nifty', 'SMA_Nifty', 'EMA_Nifty']
combined_data[features] = scaler.fit_transform(combined_data[features])

# Cross-Validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for train_index, test_index in skf.split(combined_data[features], combined_data['Target']):
    X_train, X_test = combined_data.iloc[train_index][features], combined_data.iloc[test_index][features]
    y_train, y_test = combined_data.iloc[train_index]['Target'], combined_data.iloc[test_index]['Target']

    # Reshape Data for LSTM
    X_train_reshaped = np.reshape(X_train.values, (X_train.shape[0], 1, X_train.shape[1]))
    X_test_reshaped = np.reshape(X_test.values, (X_test.shape[0], 1, X_test.shape[1]))

    # Build LSTM Model
    model = Sequential()
    model.add(LSTM(50, activation='relu', input_shape=(1, len(features))))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

    # Train the Model
    model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)

    # Predict and Evaluate
    predictions = model.predict(X_test_reshaped)
    predicted_classes = (predictions > 0.5).astype(int)
    accuracy = accuracy_score(y_test, predicted_classes)
    f1 = f1_score(y_test, predicted_classes, zero_division=0)
    roc_auc = roc_auc_score(y_test, predictions)
    conf_matrix = confusion_matrix(y_test, predicted_classes)

    print(f"Fold Accuracy: {accuracy}")
    print(f"Fold F1 Score: {f1}")
    print(f"Fold ROC-AUC: {roc_auc}")
    print(f"Confusion Matrix:\n{conf_matrix}")